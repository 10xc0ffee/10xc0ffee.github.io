<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/c0ffee.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/c0ffee.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/c0ffee.png">
  <link rel="mask-icon" href="/images/c0ffee.png" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"10xc0ffee.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":260,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":{"gitalk":{"order":-2}},"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title">
<meta property="og:url" content="https://10xc0ffee.github.io/index.html">
<meta property="og:site_name">
<meta property="og:locale" content="en_US">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://10xc0ffee.github.io/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title></title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-QSMDD3ZT3S"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-QSMDD3ZT3S","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title"></h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section">Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section">About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section">Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section">Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section">Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://10xc0ffee.github.io/2025/11/08/Neural%20Network%20and%20Deep%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | null">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/11/08/Neural%20Network%20and%20Deep%20Learning/" class="post-title-link" itemprop="url">Neural Network and Deep Learning (1)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-11-08 10:00:00" itemprop="dateCreated datePublished" datetime="2025-11-08T10:00:00-08:00">2025-11-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/tech/" itemprop="url" rel="index"><span itemprop="name">tech</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>dummy</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<p>Recently I listened a few pod casts related to AI. One is about <a
target="_blank" rel="noopener" href="https://youtu.be/pE6sw_E9Gh0?t=651">an interview with Jason
Huang</a>. Even though the boss is trying to promote his company, the
point about accelerated computing is already wide used and can
potentially reshape the entire existing applications today that based on
the general purpose computing makes sense to me. He also pointed the
direction that <a target="_blank" rel="noopener" href="https://youtu.be/pE6sw_E9Gh0?t=2060">the whole
software and hardware stack is being reshaped</a>. These all got me as
an infra engineer. So I am planning to start a blog series to record my
learning around machine learning, deep learning and AI stuff.</p>
<p>In this post, I describe the knowledge learned from <a
target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks-deep-learning">this
Coursera course</a>. I find it is beginner friendly.</p>
<h2 id="math-prerequisites">Math Prerequisites</h2>
<h4 id="matrix-dot-multiplication">Matrix Dot Multiplication</h4>
<p>Matrix dot multiplication (also known as matrix multiplication) is a
fundamental operation in linear algebra where two matrices are combined
to produce a new matrix. This operation is not element-wise; instead, it
involves taking the dot product of rows from the first matrix with
columns from the second matrix.</p>
<p>For example, if we have a matrix <span
class="math inline">\(A\)</span> of shape <span
class="math inline">\((m, n)\)</span> and a matrix <span
class="math inline">\(B\)</span> of shape <span
class="math inline">\((n, p)\)</span>, their dot product <span
class="math inline">\(C = A \cdot B\)</span> will have shape <span
class="math inline">\((m, p)\)</span>:</p>
<p><span class="math display">\[
A =
\begin{bmatrix}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22} \\
\end{bmatrix},
\quad
B =
\begin{bmatrix}
b_{11} \\
b_{21} \\
\end{bmatrix}
\]</span></p>
<p>The resulting matrix <span class="math inline">\(C\)</span> is:</p>
<p><span class="math display">\[
C = A \cdot B =
\begin{bmatrix}
a_{11}b_{11} + a_{12}b_{21} \\
a_{21}b_{11} + a_{22}b_{21} \\
\end{bmatrix}
\]</span></p>
<p>Matrix multiplication is fundamental to neural networks, serving as
the core computation within each layer. These matrix operations are
highly amenable to parallelization using SIMD (Single Instruction,
Multiple Data) and SIMT (Single Instruction, Multiple Threads)
paradigms, enabling efficient processing on modern hardware like GPUs
and specialized accelerators.</p>
<h4 id="first-derivative-and-second-derivative">First Derivative and
Second Derivative</h4>
<p>The <strong>first derivative</strong> of a function measures the rate
at which the function’s value changes as its input changes. In the
context of neural networks and optimization, the first derivative helps
us understand the <strong>slope</strong> (or gradient) of the loss
function with respect to the model parameters. When we compute gradients
during training, we are essentially finding the direction in which the
function increases or decreases most rapidly.</p>
<p>The <strong>second derivative</strong>, on the other hand, measures
how the rate of change itself changes—i.e., it gives us information
about the <strong>curvature</strong> of the function. A positive second
derivative indicates that the function is curving upwards (convex),
while a negative value indicates it’s curving downwards (concave).</p>
<h2 id="machine-learning-prerequisites">Machine Learning
Prerequisites</h2>
<h4 id="model-and-loss-function">Model and Loss Function</h4>
<p>A machine learning model can be thought of as a mathematical function
that maps input features to a predicted output. The <strong>loss
function</strong> quantifies how well the model’s predictions align with
the actual, true outcomes.</p>
<p>In supervised learning, each data sample comes with a known label
(the ground truth), denoted as <span class="math inline">\(y\)</span>.
The model’s output for the same sample is typically represented as <span
class="math inline">\(\hat{y}\)</span>. The discrepancy between the true
output and the predicted output can be measured using the loss function.
One simple example is the difference <span
class="math inline">\(\sum_{i=1}^{m} |y_i - \hat{y_i}|\)</span>, with m
denoted as the number of the samples. But more commonly, specific loss
functions such as mean squared error or cross-entropy are used,
depending on the problem type. The goal during training is to adjust the
model parameters to minimize this loss across all examples in the
dataset.</p>
<p>We denote the loss function across <span
class="math inline">\(m\)</span> data samples as <span
class="math inline">\(L(Y, \hat{Y})\)</span>, where <span
class="math inline">\(Y\)</span> and <span
class="math inline">\(\hat{Y}\)</span> are matrices of shape <span
class="math inline">\((1, m)\)</span>. Each column in <span
class="math inline">\(Y\)</span> represents the label (ground truth) for
a data sample, while each corresponding column in <span
class="math inline">\(\hat{Y}\)</span> represents the model’s
prediction.</p>
<p>The <strong>mean squared error (MSE)</strong> loss is commonly used
for regression tasks (perdict values) and can be expressed as: <span
class="math display">\[
L(Y, \hat{Y}) = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y_i})^2
\]</span></p>
<p>Alternatively, using matrix notation:</p>
<p><span class="math display">\[
L(Y, \hat{Y}) = \frac{1}{m} \operatorname{Tr}\left[(Y - \hat{Y}) (Y -
\hat{Y})^T\right]
\]</span></p>
<p>where <span class="math inline">\(\operatorname{Tr}(\cdot)\)</span>
is the trace operator, which adds up the diagonal elements of a square
matrix to produce a single scalar value. The mean squared error loss is
convex with respect to the predictions <span
class="math inline">\(\hat{Y}\)</span>, which guarantees the existence
of a unique global minimum for <span
class="math inline">\(\hat{Y}\)</span>. However, the loss may not be
convex with respect to the model parameters <span
class="math inline">\(\theta\)</span>, since <span
class="math inline">\(\hat{Y}\)</span> is typically a nonlinear function
of <span class="math inline">\(\theta\)</span>. If the relationship from
<span class="math inline">\(\theta\)</span> to <span
class="math inline">\(\hat{Y}\)</span> is itself linear (for example, if
the model is a linear function or a composition of linear functions),
then the loss remains convex with respect to <span
class="math inline">\(\theta\)</span>. Otherwise, nonlinearity can
introduce non-convexity into the optimization landscape.</p>
<p>The <strong>cross-entropy (CE)</strong> loss is widely used for
classification tasks, as it effectively measures the discrepancy between
the predicted probabilities and the true class labels.</p>
<p>For binary classification, the cross-entropy loss can be written
as:</p>
<p><span class="math display">\[
L(Y, \hat{Y}) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_{i} \log(\hat{y}_i)
+ (1 - y_i) \log(1 - \hat{y}_i) \right]
\]</span></p>
<p>Alternatively, using matrix notation:</p>
<p><span class="math display">\[
L(Y, \hat{Y}) = -\frac{1}{m} \operatorname{Tr} \left[ Y \log(\hat{Y}^T)
+ (1 - Y) \log((1 - \hat{Y})^T) \right]
\]</span></p>
<p>In this context, <span class="math inline">\(Y\)</span> represents
the true labels (usually 0 or 1), while <span
class="math inline">\(\hat{Y}\)</span> contains the predicted
probabilities for each sample, taking values between 0 and 1. Minimizing
the cross-entropy loss pushes the model’s predictions <span
class="math inline">\(\hat{y}_i\)</span> to align as closely as possible
with the actual labels <span class="math inline">\(y_i\)</span>. The
cross-entropy loss is convex with respect to <span
class="math inline">\(\hat{Y}\)</span> because its second derivative
(the Hessian matrix) is always positive.</p>
<h4 id="gradient-descent">Gradient Descent</h4>
<p><strong>Gradient descent</strong> is an optimization algorithm used
to minimize the loss function in machine learning. It works by
iteratively moving the model parameters in the direction opposite to the
gradient of the loss function (since this is the direction of steepest
decrease). Specifically, we update each parameter <span
class="math inline">\(\theta\)</span> by:</p>
<p><span class="math display">\[
\theta := \theta - \alpha \frac{\partial L}{\partial \theta}
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the
<strong>learning rate</strong> (a hyperparameter that controls the size
of each update step), and <span class="math inline">\(\frac{\partial
L}{\partial \theta}\)</span> is the gradient of the loss function <span
class="math inline">\(L\)</span> with respect to the model parameter
<span class="math inline">\(\theta\)</span>. The <span
class="math inline">\(\theta\)</span> values represent the parameters of
the machine learning model that we are trying to optimize.</p>
<h4 id="second-derivatives-role-in-gradient-descent">Second Derivative’s
Role in Gradient Descent</h4>
<p>The second derivative is important for understanding the nature of
the critical points we find using gradient descent:</p>
<ul>
<li>If the <strong>second derivative is a positive</strong>, it means
the function is <strong>convex</strong> everywhere (like a parabola
opening upwards). Gradient descent will be guaranteed to find the
<strong>global minimum</strong>.</li>
<li>If the <strong>second derivative is a negative</strong>, the
function is <strong>concave</strong> everywhere, and gradient descent
would find the global maximum.</li>
<li>However, if the <strong>second derivative varies</strong> (is not
constant), the function can have multiple local minima and maxima (local
extreme values). In such cases, gradient descent might only find a
<strong>local minimum or maximum</strong>, depending on the starting
point.</li>
</ul>
<h2 id="neural-network-introduction">Neural Network Introduction</h2>
<h4 id="notations">Notations</h4>
<figure>
<img src="./NerualNetworkNotation.jpg" alt="Neural Network Notation" />
<figcaption aria-hidden="true">Neural Network Notation</figcaption>
</figure>
<p>The input to a neural network (NN) is usually represented as a column
vector or a matrix of shape <span class="math inline">\((n_x,
1)\)</span>, where each element <span class="math inline">\(x_i\)</span>
corresponds to a specific feature. In the example above, <span
class="math inline">\(n_x = 3\)</span>, meaning there are three input
features, collectively forming the input layer.</p>
<p>This network includes an input layer, three hidden layers, and an
output layer. We denote each layer as <span
class="math inline">\(L^{[i]}\)</span>, with <span
class="math inline">\(L^{[0]}\)</span> representing the input layer and
<span class="math inline">\(L^{[4]}\)</span> the output layer (layer
<span class="math inline">\(L\)</span>). The term <span
class="math inline">\(n^{[l]}\)</span> refers to the number of neurons
(or units) in layer <span class="math inline">\(l\)</span>.</p>
<p>The input to layer <span class="math inline">\(L^{[i]}\)</span> is
simply the output from the previous layer, <span
class="math inline">\(L^{[i-1]}\)</span>. Thus, the input for layer
<span class="math inline">\(l\)</span> is expressed as a matrix <span
class="math inline">\(a^{[l-1]}\)</span> with shape <span
class="math inline">\((n^{[l-1]}, 1)\)</span>. Each neuron in the
network applies a linear transformation to its inputs, followed by a
non-linear activation function. This activation function injects
non-linearity at each layer, enabling the neural network to learn
complex patterns and solve more challenging tasks.</p>
<p>At each layer <span class="math inline">\(l\)</span>, the linear
transformation is computed as:</p>
<p><span class="math display">\[
z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}
\]</span></p>
<p>Here, <span class="math inline">\(W^{[l]}\)</span> represents the
weights connecting layer <span class="math inline">\(l-1\)</span> to
layer <span class="math inline">\(l\)</span> and typically has shape
<span class="math inline">\((n^{[l]}, n^{[l-1]})\)</span>. The bias
<span class="math inline">\(B^{[l]}\)</span> is generally a vector of
shape <span class="math inline">\((n^{[l]}, 1)\)</span>. These will make
<span class="math inline">\(z^{[l]}\)</span> a matrix of shape <span
class="math inline">\((n^{[l]}, 1)\)</span>.</p>
<p>The activation for layer <span class="math inline">\(l\)</span> is
then calculated by applying a (generally nonlinear) activation function
<span class="math inline">\(g\)</span>:</p>
<p><span class="math display">\[
a^{[l]} = g(z^{[l]})
\]</span></p>
<p>A NN can be viewed as a universal function approximator. According to
the Universal Approximation Theorem (UAT), a standard feedforward neural
network with just one hidden layer can approximate <strong>any
continuous function</strong> to any desired level of accuracy, provided
the hidden layer contains a sufficient number of neurons. For
non-continuous functions, which may have discontinuities (such as jumps
or points of infinite value), NNs cannot reproduce these exact
discontinuities. However, in practical applications, NNs can closely
approximate such behaviors by using their ability to model the
surrounding continuous regions. This makes neural networks highly
effective for real-world scenarios, where absolute precision near
discontinuities is often not required.</p>
<h4 id="activation-function">Activation Function</h4>
<p>The <strong>sigmoid function</strong> produces outputs between <span
class="math inline">\((0, 1)\)</span>, which makes it particularly
well-suited for binary classification tasks. It is commonly used as the
activation function in the output layer of neural networks when modeling
probabilities. <span class="math display">\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]</span></p>
<p>The <strong>hyperbolic tangent</strong> (tanh) function produces
outputs in the range <span class="math inline">\((-1, 1)\)</span>. It is
often favored over the sigmoid in hidden layers because it is
zero-centered—meaning its outputs have a mean of zero—which tends to
improve convergence during training. This property allows gradients to
flow in both positive and negative directions, helping different
parameters update more independently. In contrast, the sigmoid’s outputs
are always positive, potentially causing less efficient updates because
it doesn’t allow each parameter move independently.</p>
<p><span class="math display">\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]</span></p>
<p>The <strong>Rectified Linear Unit (ReLU)</strong> activation function
is extensively used in modern neural networks due to its simplicity,
computational efficiency, and its role in mitigating the vanishing
gradient problem. Unlike sigmoid or tanh functions, ReLU outputs zero
for any negative input and behaves as the identity function for positive
inputs. This property preserves gradients for positive values, enabling
faster and more stable learning.</p>
<p>However, ReLU can suffer from issues such as the “dying ReLU”
problem, in which a neuron becomes inactive and outputs zero for all
subsequent inputs. While the sigmoid function experiences vanishing
gradients for extreme values, ReLU’s main challenge is neuron
inactivation for consistently negative inputs. To help address such
training instabilities, modern networks often employ <strong>Batch
Normalization</strong> (<a href="">need follow up later</a>), which
helps maintain healthier distributions of activations across layers.</p>
<p>Despite its drawbacks, ReLU remains popular because it effectively
solve vanishing gradients and promotes sparsity in network
representations by deactivating some neurons, which can also provide a
form of regularization.</p>
<p><span class="math display">\[
\operatorname{ReLU}(x) = \max(0, x)
\]</span></p>
<p>However, ReLU can lead to the “dying ReLU” problem where some neurons
output only zero and stop learning. The <strong>Leaky ReLU</strong>
modifies this by allowing a small, non-zero gradient when the input is
negative: <span class="math display">\[
\operatorname{LeakyReLU}(x) =
\begin{cases}
x &amp; \text{if } x \ge 0 \\
\alpha x &amp; \text{if } x &lt; 0
\end{cases}
\]</span> where <span class="math inline">\(\alpha\)</span> is a small
positive constant (e.g., 0.01).</p>
<h4 id="training">Training</h4>
<p>Training a NN involves <span class="math inline">\(k\)</span>
iterations of forward propogations and backward propogations.</p>
<p>A forward propogation aims to calculate the loss at the output layer
based on the model parameters of the current layer. Then a backward
propogation will update gradients from the output layer all the way back
to the first layer (note that this is not input layer). Assuming there
are <span class="math inline">\(m\)</span> labeled data samples, the
forward propogation can be denoted as the following matrix operations.
We will extend the notations from “#### Notations” section. For example,
<span class="math inline">\(A^{[0]}\)</span> is the labeled data
samples, shapes a matrix of shape <span class="math inline">\((n_x,
m)\)</span>. Similarly, at layer 1, <span class="math inline">\(Z^{[1]}
= W^{[1]}A^{[0]}+\operatorname{broadcast}(b^{[1]})\)</span>. To
generalize it to layer <span class="math inline">\(l\)</span>, we
have:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; W^{[l]}.shape = (n^{[l]}, n^{[l-1]})    \\
&amp; A^{[l-1]}.shape = (n^{[l-1]}, m)        \\
&amp; b^{[l]}.shape = (n^{[l]}, 1)            \\
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp; Z^{[l]} = W^{[l]}A^{[l-1]} + \operatorname{broadcast}(b^{[l]}) \\
&amp; Z^{[l]}.shape = (n^{[l]}, m)
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp; A^{[l]} = g(Z^{[l]}) \\
&amp; A^{[l]}.shape = (n^{[l]}, m)
\end{aligned}
\]</span></p>
<p>At each layer, the activation function <span
class="math inline">\(g\)</span> could be different.</p>
<p>Backward propagation (backpropagation) is the process by which neural
networks update their parameters <span
class="math inline">\(W^{[l]}\)</span> and <span
class="math inline">\(B^{[l]}\)</span> at each layer <span
class="math inline">\(l\)</span> using gradient descent. The key goal is
to compute the gradients <span class="math inline">\(\frac{\partial
L}{\partial W^{[l]}}\)</span> (denoted as <span
class="math inline">\(dW^{[l]}\)</span>) and <span
class="math inline">\(\frac{\partial L}{\partial B^{[l]}}\)</span>
(denoted as <span class="math inline">\(dB^{[l]}\)</span>), iteratively
from the output layer back to the first layer. Assuming the use of the
cross-entropy loss function:</p>
<p><span class="math display">\[
\begin{aligned}
L(Y, \hat{Y}) = -\frac{1}{m} \operatorname{Tr} \left[ Y \log(\hat{Y}^T)
+ (1 - Y) \log((1 - \hat{Y})^T) \right]
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\hat{Y} = A^{[L]}\)</span>. At each
layer <span class="math inline">\(l\)</span>, the gradients are computed
as follows:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; dA^{[l]} = \text{gradient of the loss with respect to } A^{[l]} \\
&amp; dZ^{[l]} = dA^{[l]} \circ g&#39; (Z^{[l]}) \quad
\text{(element-wise product with activation derivative)} \\
&amp; dW^{[l]} = \frac{1}{m} dZ^{[l]} (A^{[l-1]})^T \\
&amp; db^{[l]} = \frac{1}{m} \operatorname{sum}(dZ^{[l]}, \text{axis}=1,
\text{keepdims=True}) \\
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(\circ\)</span> denotes element-wise
multiplication, and <span class="math inline">\(g&#39;\)</span> is the
derivative of the activation function. These gradients are then used to
update the weights and biases during training.</p>
<h4 id="inferencing">Inferencing</h4>
<p>Inference consists of a single forward propagation through the
network. For example, in a binary classification problem, the output
layer typically uses a sigmoid activation function, resulting in an
output <span class="math inline">\(a^{[L]}\)</span> with shape <span
class="math inline">\((1, 1)\)</span>. This output represents the
predicted probability, and the neural network makes its decision based
on this value.</p>
</body>
</html>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://10xc0ffee.github.io/2021/06/27/Life%20Beyond%20Distributed%20Transactions:%20An%20Apostate's%20Opinion/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | null">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/27/Life%20Beyond%20Distributed%20Transactions:%20An%20Apostate's%20Opinion/" class="post-title-link" itemprop="url">Note on Life Beyond Distributed Transactions</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-06-27 10:00:00" itemprop="dateCreated datePublished" datetime="2021-06-27T10:00:00-07:00">2021-06-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/tech/" itemprop="url" rel="index"><span itemprop="name">tech</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>dummy</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<h2 id="problem-statement">Problem Statement</h2>
<p>Real-world application data grows fast. Those data will have to be
repartitioned over time. Global serializability then becomes a problem.
This introduces challenges in maintaining global serializability.
However, user applications often struggle to achieve good performance
using distributed transaction algorithms like 2PC (as noted by the
author in 2007).</p>
<p>As a result, user applications typically implement custom solutions
to coordinate distributed procedures tailored to their systems. This
paper discusses a model and guidelines for users to handle data scaling
challenges and for infrastructure developers to design new
frameworks.</p>
<h2 id="assumption">Assumption</h2>
<ul>
<li>Application data grows rapidly, but the types of data are
limited.</li>
<li>Application developers write scale-agnostic code.</li>
<li>Infrastructure provides multiple disjoint scopes of transaction
serializability (e.g., single-machine atomicity).</li>
<li>For messaging, most applications prefer (or infrastructure typically
provides) at-least-once delivery but at-most-once acceptance.</li>
<li>Exactly-once semantics, akin to the TCP protocol, work well for
temporary, non-persistent messages in short-lived processes. However,
achieving exactly-once semantics for durable, long-lived messages is
significantly more complex. Refer to <a
target="_blank" rel="noopener" href="https://web.stanford.edu/~ouster/cgi-bin/papers/rifl.pdf">RIFL</a>
to see an implementation example).</li>
</ul>
<h2 id="pattern">Pattern</h2>
<p>Given these assumptions, the author proposes a design pattern for a
scale-agnostic application layer.</p>
<p><strong>Entity</strong></p>
<p>An entity is an addressable user data type where atomicity is
maintained. Each entity can be identified by a unique key. The author
emphasizes that data referenced by secondary and primary indices
typically belong to different entities due to performance considerations
(e.g., coordination costs).</p>
<p><strong>Message</strong></p>
<p>The message is used to communicate across entities. When the update
of entity A and the update of entity B are related (e.g. causality),
there will be messages between A to B. Messaging from entity A to entity
B should be outside the transaction of either entity A or B. (For
example, a user submits an order. The application updates the order
table row A. If that succeeds, then it needs to modify the balance table
row B to charge the user. That may require a message that relates A to
B. The messaging procedure should be outside transaction of A to prevent
B receives the message before the transaction of A is committed).</p>
<p><strong>Activity</strong></p>
<p>The effect of a Message must be idempotent under the assumption of
at-least-once-delivery. So the downstream entity needs to maintain some
<strong>state</strong> to track those messages. That state is a part of
an activity. An application can define more relationships
(i.e. activities) between the two entities. The author thinks if there
is messaging between two entities, there should be an activity tracking
these two entities. Under the assumption of limited user data types, the
number of activities is also limited. So with this pattern, it doesn’t
mean application developers have to write a lot of code. Another
important state of the activity is the <strong>uncertainty</strong>. For
example, we are going to charge the bank account, but before we do that
we don’t know how much money is in the account. That is uncertainty. The
activity needs to save the state of uncertainty. The goal is to resolve
the uncertainty. Considering if we are implementing a distributed
transaction, normally the uncertainty is resolved along with the
“locking”, either pessimistic or optimistic. At the user application
level, we track them as tentative operations, confirmations (like
transaction committed), and cancellations (like transaction
aborted).</p>
<p><strong>Workflow</strong></p>
<p>Application business logic will be composited by different messages
and related activities (which are like building blocks).</p>
<h3 id="my-thoughts">My Thoughts</h3>
<p>In 2007, when NoSQL databases were gaining traction, this paper
provided a design pattern for implementing complex business logic atop
NoSQL databases with single-row atomicity. However, implementing such
logic remains more challenging compared to using distributed RDBMSs.
Applications must handle activities and their states, build state
machines to respond to activity outcomes, and address distributed
failures—an inherently labor-intensive process.</p>
<p>Many assumptions in this paper no longer hold in 2021. Today, we have
robust solutions like exactly-once semantic messaging and global
transactions, which simplify application development. Nevertheless, this
pattern remains relevant in certain scenarios.</p>
<p>But in certain areas, this pattern will still be useful.</p>
<p>The first use case I can think of is when the workflow is accessing
multiple services/databases. And only accessing a single service
supports the atomicity. In some real-world business logics, a human can
be an entity as well and can be involved in the workflow (The human
interaction will usually take a very long time to process). So you have
to break a global transaction into smaller transactions. I came across
[7 Use Cases in 7 Minutes Each]
(https://www.youtube.com/watch?v=sXGlQruUrWE), which talks about the
real use cases by using AWS SWF. AWS SWF has many similar ideas and
abstractions to help application developers focus on the business logic,
without worrying about workload scaling (not data scaling) and
distributed failures. SWF has the concept of activity worker and
workflow worker, where the activity is like a simple step of business
logic, the workflow is like a decision-maker/coordinator of different
activities’ execution. Application developers just need to break the
business logic into activities and workflows. All other messaging, state
durability, and distributed failures are handled by SWF.</p>
</body>
</html>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://10xc0ffee.github.io/2018/07/16/leveldb_summarize/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | null">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/07/16/leveldb_summarize/" class="post-title-link" itemprop="url">Leveldb Notes</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-07-16 10:00:00" itemprop="dateCreated datePublished" datetime="2018-07-16T10:00:00-07:00">2018-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/tech/" itemprop="url" rel="index"><span itemprop="name">tech</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>dummy</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<h3 id="leveldb-简介">Leveldb 简介</h3>
<p>Leveldb是一个单机key-value数据库library。Comparator，filter，cache，以及各种性能相关的参数都可以在创建DB的时候由参数传入。基于LSM，leveldb倾向于优化写请求，更适合写请求远大于读请求的负载。</p>
<h3 id="leveldb-架构">Leveldb 架构</h3>
<p>|==============================DB==============================|</p>
<p>|====VersionSet====|</p>
<p>|====CurVersion====||====MemTable====||====ImmTable====||====WAL====|</p>
<p>|====TableCache====||====BlockCache====|</p>
<p>|=============================Disk=============================|</p>
<p>Leveldb架构如上，由五六个重要的数据结构组成。DB提供用户接口，所有的存在Snapshot，Read，Write，Delete，Iterate对外接口都在这层提供。VersionSet追踪了所有SSTable的历史变动。CurVersion包含当前SSTable的索引，所有读操作访问table
cache都要先经过它来定位SSTable。WAL记录写操作的oplog。MemTable使用跳表提供了快速写和无锁读特性。TableCache默认使用LRU将SSTable映射到内存中，提供快速的查找和遍历。</p>
<p>其中最中间的这层构成了整个数据库当前的视图。对于Leveldb的三种operations：</p>
<ul>
<li>Write主要涉及WAL的append，MemTable的insert</li>
<li>Read主要涉及MemTable和ImmTable的get，以及CurVersion的table
find和TableCache的get</li>
<li>Compaction主要涉及ImmTable的serialize或SSTables的多路归并，和VersionSet的meta
change</li>
</ul>
<h3 id="writes-in-leveldb">Writes in leveldb</h3>
<p>Leveldb的写操作包括Upsert和Delete两种类型。和其他log系统一样，写操作将操作（WriteBatch）写到log
file之后就返回用户成功。后台的Compaction
task负责Immtable持久化和多sstable的归并。数据同时会在内存以跳表的形式存在。</p>
<p>前台写WriteBatch会被加入FIFO，队首所在用户线程负责batch这些写请求，并通过notify其他用户线程同一批落盘的写请求的完成。这种做法降低了写请求的平均等待时间，原因如下，</p>
<ul>
<li>每次只有一个线程在写log
file，这个过程不需要加锁，给新的Write请求进入队列的空隙。</li>
<li>batch write对于较小的log
record连续append来说，更给有利于提高磁盘吞吐。</li>
</ul>
<p>后台Compaction对前台写有负反馈。为了保证memtable的性能，其占用内存是有软硬threshold的。当达到软threshold的时候，前台写会sleep
1秒, 以让出CPU等待immtable的Compaction。这种思路有利于避免长尾。</p>
<h3 id="compaction-策略">Compaction 策略</h3>
<p>粗略的想来，Compaction策略要能优化下面几点，</p>
<ul>
<li>Compaction的触发时机要均匀，以均摊磁盘IO，减少对前台的影响。</li>
<li>Compaction要减少上下层间的overlapp，减少对Read操作的影响。</li>
</ul>
<p>另外leveldb
LSM，level越高允许的文件越多，应该是隐含了倾向于越新写的越应该置于能快速访问到的位置（可以想一下KV插入越早，其所在的层久越高，也越高概率和低层有overlap）。</p>
<p>Compaction策略每一家都不一样，这部分我没怎么看懂，进一步理解需要研究下测试方法和有没有数学形式的分析。</p>
<h3 id="reads-in-leveldb">Reads in leveldb</h3>
<p>Leveldb中的读是snapshot读。最长的读路径需要经过Memtable-&gt;ImmTable-&gt;CurVersion
SSTables。每个Version视图保留了每层的SSTable Key
Range，通过二分查找可以定位出Key可能所在的SSTable。之后Leveldb的做法是通过VersionSet中的TableCache的Get接口去拿到value，顺便加入新的TableCache和BlockCache
Entry if possible。</p>
<h3 id="snapshot-实现">Snapshot 实现</h3>
<p>Leveldb支持MVCC，对外可以提供snapshot
isolation。其中Version即是写请求产生的Sequence
Id。用户在DB的生命周期内创建了某个SeqId的snapshot，对于DB来说，仅代表Compaction不会merge并删除该SeqId之后的sstable。</p>
<h3 id="关于leveldb-数据安全">关于Leveldb 数据安全</h3>
<p>涉及fdatasync和fsync的使用。POSIX说fdatasync会sync和fd相关的data和保证数据准确性的meta。我发现Leveldb使用的是fdatasync
if possible。 要注意SyncDirIfManifest，Manifest
文件持久化了新加入的sstable并去除了多余的sstable，为了保证Manifest的data
sync完成后其中记录的sstable都存在，需要在之前调用父目录的fdatasync。
另外unlink和rename在filesystem层不一定保序，这导致需要SetCurrentManifest在rename之后调用SyncDirIfManifest，防止redundant的sstable被删除而Manifest却没更新。</p>
<h3 id="leveldb代码">Leveldb代码</h3>
<p>Leveldb的代码比较容易理解，特别是几个抽象Memtable, VersionSets,
Version, Iterator,
使整个架构很清晰。对函数调用前置条件的注释和Assertion很值得借鉴。</p>
<p>不好阅读的地方也有。VersionSets和CurrentVersion，TableCache的互相引用，DB
Mutex作为参数横跨很长的调用链。这些导致我对一些细节没法很清晰的察觉，需要进一步熟悉代码才行。</p>
</body>
</html>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://10xc0ffee.github.io/2018/04/21/raft_notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | null">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2018/04/21/raft_notes/" class="post-title-link" itemprop="url">RAFT Notes</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2018-04-21 10:00:00" itemprop="dateCreated datePublished" datetime="2018-04-21T10:00:00-07:00">2018-04-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/tech/" itemprop="url" rel="index"><span itemprop="name">tech</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>dummy</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<h3 id="leader-election-是如何进行的">Leader election
是如何进行的？</h3>
<p>raft的leader
election阶段类似使用paxos算法谋求一个共同的值(leader)。大致阶段如下：</p>
<ol type="1">
<li>每个Proposer初始得到一个election timeout。这个election
timeout是在一个范围内随机的。</li>
<li>若max election
timeout超时未收到其他proposal，则向quorum发布leader是自己的proposal。</li>
<li>若得到majority acceptor认可，则成为leader。</li>
</ol>
<p>每个proposal都附带一个sequence
number，在raft中称为term。term标志新一轮election的产生。算法允许不同proposer使用相同term
propose the leader。</p>
<h3
id="为什么所有写操作包括leader-election都是majority-based">为什么所有写操作（包括leader
election）都是majority based？</h3>
<p>A majority got newest value =&gt; any majority group contains the
newest value =&gt; some minority group don’t have the newest value</p>
<p>Then we can choose to stop the service when there are only minority
accessible. In the other hand, service can be online only when there are
majoirty cab be accessible. That’s basically pigenon hole principle.</p>
<p>由于我们没法知道全局中有哪些replica含有最新的数据，是否能做leader都是通过比较判断的。如果我们有全局的信息实时记录谁有最新的数据，我们就能打破majority的限制。一些基于failure
detector的协议就是这么做的。</p>
<h3 id="如何保证leader的uniqueness">如何保证leader的uniqueness</h3>
<p>raft算法本身并不维护leader的唯一。raft只维护在相同term下只会产生一个leader。所以算法是会出现同时存在多个replica认为自己是leader的情况。</p>
<ul>
<li>对于写请求，由于只有newest term
leader拥有majority的ISR，所以写请求只会在这个leader上成功。</li>
<li>对于读请求，可能读到旧值。</li>
</ul>
<p>整体上基本的raft算法能提供sequential
consistency，却达不到linearizability。对于有些需求外部时间或者因果一致的应用，有的会增加了lease保证leader的uniqueness，有的在读请求上做文章，保证读的时候还是leader否则读失败。但是有得必有失，这种extension增加了算法对时间的依赖，即分布式环境中time
drift对应用的影响。</p>
<h3
id="新上任的leader什么时候可以对外服务">新上任的Leader什么时候可以对外服务？</h3>
<p>在raft中，成为leader并不意味这你确定所有数据已经commit，只能确定committed数据和最新数据（to-commit数据）是你的数据的子集。所以raft中新leader不能直接服务，需要经过数轮AppendEntries
commit一个当前term的占位entry才行
（我有个疑问为什么必须提交一个真实的当前term的dummy
entry。空的AppendEntries作为心跳也可以把之前lag的entries同步吧？虽然这样有违raft
commit的原则，但是我觉得这个时候这些previous
term的entries是可以commit的了）。</p>
<h3 id="谁可以commit数据">谁可以commit数据？</h3>
<p>Leader可以commit数据，不仅仅是leader with newest term。old
leader可能会commit数据并告知clients。raft保证old
leader的后继leader肯定会commit一遍同样的entries。最新leader也同样包含相同的entries。</p>
</body>
</html>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://10xc0ffee.github.io/2016/06/25/storage%20note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | null">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2016/06/25/storage%20note/" class="post-title-link" itemprop="url">Storage Notes</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2016-06-25 10:00:00" itemprop="dateCreated datePublished" datetime="2016-06-25T10:00:00-07:00">2016-06-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/tech/" itemprop="url" rel="index"><span itemprop="name">tech</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>dummy</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<h3 id="storage-system-design-considerations">Storage System Design
Considerations</h3>
<p>A storage system, used to save data, is often a fundamental service
for many backend applications. Designing a storage system involves three
critical considerations: <strong>Data Safety</strong>,
<strong>Performance</strong>, and <strong>Availability</strong>.</p>
<h4 id="data-safety">Data Safety</h4>
<p>Data safety is the most important aspect in most scenarios. The
system must make every effort to protect user data, as any loss of data
is unacceptable.</p>
<h4 id="availability">Availability</h4>
<p>While availability may not always be an immediate concern, it remains
essential for system stability. In modern storage systems, data is
typically replicated across multiple nodes. If one node becomes
unavailable, other nodes can handle requests. However, poor availability
can lead to system instability, manifesting as increased latency or
reduced throughput when alternate nodes are used.</p>
<p>One of the key factors affecting availability is the speed of
stopping and restarting nodes, which impacts how quickly the system can
recover from failures.</p>
<h4 id="performance">Performance</h4>
<p>Performance is typically measured in two dimensions:
<strong>latency</strong> (how quickly a system responds) and
<strong>throughput</strong> (the amount of data the system processes).
Performance is the second most important consideration and is influenced
by data safety and availability. Optimizing performance requires careful
design choices, ranging from hardware selection to user
requirements.</p>
<h5 id="network-interface-card-nic">Network Interface Card (NIC)</h5>
<p>The NIC is a crucial determinant of network speed, with options like
1-Gb, 10-Gb, and Infiniband affecting overall system performance. For
example, with 1-Gb NICs, two approaches are commonly used for
replicating data to ensure safety:</p>
<ol type="1">
<li><strong>Star-Write</strong>: Sends all three replicas simultaneously
from one node to others. This approach minimizes latency but can
saturate the client’s bandwidth, reducing throughput.</li>
<li><strong>Chain-Write</strong>: Sends the first replica to one node,
which then forwards it to others. This approach maximizes throughput by
utilizing bandwidth across multiple nodes, but it introduces higher
latency due to forwarding delays.</li>
</ol>
<p>To mitigate these challenges, <strong>two-phase commit</strong>
strategies can be employed: - <strong>Asynchronous writes</strong>:
Writes are first completed asynchronously and then retried until
committed. - <strong>Commit phase</strong>: Ensures data integrity while
balancing throughput and latency.</p>
<p>With the widespread adoption of 10-Gb NICs, Star-Write can now
achieve both low latency and high throughput, especially when integrated
with a two-phase commit model.</p>
<h5 id="disk">Disk</h5>
<p>Disks are the backbone of storage systems, and their
configurations—such as HDD, SSD, SAS, RAID, and combinations—can
significantly influence design and performance. Each choice has
trade-offs in cost, speed, and durability.</p>
<h5 id="memory-hierarchy">Memory Hierarchy</h5>
<p>Modern computer architecture relies heavily on a memory hierarchy.
Efficient use of cache significantly affects performance. Cache misses
introduce latency, making locality optimization critical.</p>
<p><strong>Types of Locality</strong>:</p>
<pre><code>**Spatial Locality**: Improves by ensuring data close in memory is cached together.
- Larger cache sizes.
- Memory indexing structures for data continuity.
- Optimized binary layouts to improve instruction cache locality.

**Temporal Locality**: Improves by reusing data or instructions over time.
- Prefetching techniques.
- Cache replacement algorithms.
- Modular design: Breaking requests into self-contained modules connected by queues. This design enables the system to group similar operations, improving temporal locality. Advanced implementations like **stagedDB** ensure proper context-switching within modules, even when cache capacity is limited.</code></pre>
<p>At the microarchitecture level, out-of-order processors help tolerate
instruction misses by executing subsequent instructions. Many of these
techniques rely on accurate prediction, which is constrained by
unpredictable data sets and request patterns. However, good predictions
can yield substantial performance benefits.</p>
<h4 id="stability-requirements">Stability Requirements</h4>
<p><strong>Multitenancy</strong><br />
When multiple customers share a storage system, ensuring fairness is
critical. The system may implement priority levels to safeguard
high-priority requests without starving low-priority ones. Effective
multitenancy involves: - Fine-grained controls for individual requests.
- End-to-end priority management across jobs and tenants.</p>
<p><strong>Graceful Degradation</strong><br />
Graceful degradation ensures that when the system approaches maximum
capacity, throughput remains steady, and latency increases
proportionally. This requires the system to implement throttling
mechanisms to reject excessive requests gracefully.</p>
</body>
</html>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder"></span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">Don't Drink <a href="http://10xc0ffee.github.io/">C0ffee</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM=" crossorigin="anonymous">


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"client_id":"Ov23li2yAgo3qKZQAjlD","client_secret":"f0be8a2aaa1f183bd266d968866febe47de9b6ef","github_id":"10xc0ffee","admin_user":"10xc0ffee","repo":"10xc0ffee.github.io","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"en","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"d1546d731a9f30cc80127d57142a482b"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
